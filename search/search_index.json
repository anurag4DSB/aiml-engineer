{"config":{"lang":["en"],"separator":"[\\s\\-\\.]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"aiml.engineer","text":"<p>A knowledge base for AI/ML engineering - notes, labs, projects, and blog posts.</p>"},{"location":"#sections","title":"Sections","text":"<ul> <li>Notes - Concise explanations and concepts</li> <li>Labs - Hands-on experiments</li> <li>Projects - Case studies and builds</li> <li>Posts - Long-form essays</li> <li>Reading - Paper summaries and critiques</li> </ul>"},{"location":"#recent","title":"Recent","text":"<ul> <li>VLIW SIMD Architecture - Understanding VLIW architectures and SIMD optimization techniques</li> </ul>"},{"location":"labs/","title":"Labs","text":"<p>Hands-on experiments and Jupyter notebooks.</p>"},{"location":"notes/","title":"Notes","text":"<p>Concise explanations, definitions, and concepts.</p> <ul> <li>VLIW SIMD Architecture - Understanding VLIW architectures and SIMD optimization</li> </ul>"},{"location":"notes/vliw-simd/","title":"VLIW SIMD Architecture - Learning Guide","text":"<p>This documentation covers the key concepts needed to understand and optimize for VLIW SIMD architectures.</p>"},{"location":"notes/vliw-simd/#documents","title":"Documents","text":"<ol> <li> <p>Scalar vs Vector - Understanding the difference between processing one value vs eight values at once</p> </li> <li> <p>ALU and Engines - What ALU means and the five execution engines in this architecture</p> </li> <li> <p>VLIW Architecture - How Very Long Instruction Word architectures work</p> </li> <li> <p>Instruction Set Reference - Complete list of available instructions</p> </li> <li> <p>Optimization Strategies - Techniques to reduce cycle count</p> </li> </ol>"},{"location":"notes/vliw-simd/#quick-reference","title":"Quick Reference","text":""},{"location":"notes/vliw-simd/#performance-targets","title":"Performance Targets","text":"Cycles Description 147,734 Baseline (unoptimized) 18,532 Updated starting point 1,790 Best human ~2hr / Claude Opus 4.5 casual 1,487 Impressive threshold 1,363 Best known"},{"location":"notes/vliw-simd/#engine-slot-limits","title":"Engine Slot Limits","text":"Engine Slots/Cycle Purpose alu 12 Scalar arithmetic valu 6 Vector arithmetic (8 elements) load 2 Memory reads store 2 Memory writes flow 1 Control flow"},{"location":"notes/vliw-simd/#key-constants","title":"Key Constants","text":"<ul> <li>VLEN = 8 - Vector length (elements per vector operation)</li> <li>SCRATCH_SIZE = 1536 - Available scratch space (32-bit words)</li> <li>Batch size = 256 - Items processed per round</li> <li>Rounds = 16 - Number of iterations</li> </ul>"},{"location":"notes/vliw-simd/alu-and-engines/","title":"ALU and Execution Engines","text":""},{"location":"notes/vliw-simd/alu-and-engines/#alu-arithmetic-logic-unit","title":"ALU = Arithmetic Logic Unit","text":"<p>A fundamental CPU component that's been around since the 1940s. It's the part of a processor that does math and logic operations.</p>"},{"location":"notes/vliw-simd/alu-and-engines/#what-alu-handles","title":"What ALU Handles","text":"<ul> <li>Arithmetic: <code>+</code>, <code>-</code>, <code>*</code>, <code>//</code>, <code>%</code></li> <li>Logic: <code>&amp;</code> (AND), <code>|</code> (OR), <code>^</code> (XOR)</li> <li>Shifts: <code>&lt;&lt;</code>, <code>&gt;&gt;</code></li> <li>Comparisons: <code>&lt;</code>, <code>==</code></li> </ul> <p>Every CPU has an ALU. Your laptop, phone, even a calculator - all have ALUs.</p>"},{"location":"notes/vliw-simd/alu-and-engines/#valu-vector-alu","title":"VALU = Vector ALU","text":"<p>The vector version of ALU - does the same operations but on 8 values simultaneously.</p>"},{"location":"notes/vliw-simd/alu-and-engines/#all-engines-in-this-architecture","title":"All Engines in This Architecture","text":"Name Full Name Slots/Cycle Purpose <code>alu</code> Arithmetic Logic Unit 12 Scalar math/logic <code>valu</code> Vector ALU 6 Vector math/logic (8 elements each) <code>load</code> Load Unit 2 Read from memory into scratch <code>store</code> Store Unit 2 Write from scratch to memory <code>flow</code> Control Flow 1 Branches, jumps, selects, halt"},{"location":"notes/vliw-simd/alu-and-engines/#what-slots-per-cycle-means","title":"What \"Slots per Cycle\" Means","text":"<p>Each engine can execute multiple operations per cycle (up to its slot limit):</p> <pre><code># This is ONE cycle - all 3 ALU operations happen simultaneously\n{\n    \"alu\": [\n        (\"+\", 0, 1, 2),   # scratch[0] = scratch[1] + scratch[2]\n        (\"-\", 3, 4, 5),   # scratch[3] = scratch[4] - scratch[5]\n        (\"*\", 6, 7, 8),   # scratch[6] = scratch[7] * scratch[8]\n    ]\n}\n</code></pre> <p>The ALU engine has 12 slots, so you could pack up to 12 independent ALU operations in a single cycle.</p>"},{"location":"notes/vliw-simd/alu-and-engines/#why-multiple-engines-matter","title":"Why Multiple Engines Matter","text":"<p>Different engines work in parallel. In one cycle you can:</p> <ul> <li>Do 12 ALU operations AND</li> <li>Do 6 VALU operations AND</li> <li>Do 2 loads AND</li> <li>Do 2 stores AND</li> <li>Do 1 flow operation</li> </ul> <p>Maximum theoretical throughput per cycle: 23 operations.</p> <p>The baseline implementation uses ~1 operation per cycle. That's why there's so much room for optimization.</p>"},{"location":"notes/vliw-simd/alu-and-engines/#real-cpus-use-these-terms","title":"Real CPUs Use These Terms","text":"<ul> <li>Intel/AMD x86: Has multiple ALUs per core</li> <li>GPUs (NVIDIA/AMD): Thousands of ALUs for parallel processing</li> <li>ARM: ALU is part of the core pipeline</li> </ul> <p>The terminology in this challenge mirrors real processor architecture.</p>"},{"location":"notes/vliw-simd/instruction-set/","title":"Instruction Set Reference","text":"<p>Complete reference for all instructions available in this architecture.</p>"},{"location":"notes/vliw-simd/instruction-set/#instruction-format","title":"Instruction Format","text":"<p>Every instruction is a tuple: <code>(engine, (operation, ...operands))</code></p> <p>Most operands are scratch addresses (not values). The scratch space is like a large register file with 1536 slots.</p>"},{"location":"notes/vliw-simd/instruction-set/#alu-engine-12-slotscycle","title":"ALU Engine (12 slots/cycle)","text":"<p>Scalar arithmetic and logic operations.</p> <pre><code>(\"alu\", (op, dest, src1, src2))\n# scratch[dest] = scratch[src1] OP scratch[src2]\n</code></pre>"},{"location":"notes/vliw-simd/instruction-set/#available-operations","title":"Available Operations","text":"Op Description Example <code>+</code> Addition <code>scratch[0] = scratch[1] + scratch[2]</code> <code>-</code> Subtraction <code>scratch[0] = scratch[1] - scratch[2]</code> <code>*</code> Multiplication <code>scratch[0] = scratch[1] * scratch[2]</code> <code>//</code> Integer division <code>scratch[0] = scratch[1] // scratch[2]</code> <code>%</code> Modulo <code>scratch[0] = scratch[1] % scratch[2]</code> <code>&amp;</code> Bitwise AND <code>scratch[0] = scratch[1] &amp; scratch[2]</code> <code>\\|</code> Bitwise OR <code>scratch[0] = scratch[1] \\| scratch[2]</code> <code>^</code> Bitwise XOR <code>scratch[0] = scratch[1] ^ scratch[2]</code> <code>&lt;&lt;</code> Left shift <code>scratch[0] = scratch[1] &lt;&lt; scratch[2]</code> <code>&gt;&gt;</code> Right shift <code>scratch[0] = scratch[1] &gt;&gt; scratch[2]</code> <code>&lt;</code> Less than (returns 0 or 1) <code>scratch[0] = 1 if scratch[1] &lt; scratch[2] else 0</code> <code>==</code> Equal (returns 0 or 1) <code>scratch[0] = 1 if scratch[1] == scratch[2] else 0</code>"},{"location":"notes/vliw-simd/instruction-set/#valu-engine-6-slotscycle","title":"VALU Engine (6 slots/cycle)","text":"<p>Vector operations on VLEN=8 contiguous elements.</p> <pre><code>(\"valu\", (op, dest, src1, src2))\n# scratch[dest:dest+8] = scratch[src1:src1+8] OP scratch[src2:src2+8]\n</code></pre>"},{"location":"notes/vliw-simd/instruction-set/#available-operations_1","title":"Available Operations","text":"<p>Same as ALU (<code>+</code>, <code>-</code>, <code>*</code>, <code>//</code>, <code>%</code>, <code>&amp;</code>, <code>|</code>, <code>^</code>, <code>&lt;&lt;</code>, <code>&gt;&gt;</code>, <code>&lt;</code>, <code>==</code>) but operates on 8 elements.</p>"},{"location":"notes/vliw-simd/instruction-set/#special-broadcast","title":"Special: Broadcast","text":"<pre><code>(\"valu\", (\"vbroadcast\", dest, src))\n# scratch[dest:dest+8] = [scratch[src], scratch[src], ..., scratch[src]]\n# Copies one scalar value into all 8 vector slots\n</code></pre>"},{"location":"notes/vliw-simd/instruction-set/#load-engine-2-slotscycle","title":"Load Engine (2 slots/cycle)","text":"<p>Read from memory or load constants.</p>"},{"location":"notes/vliw-simd/instruction-set/#load-constant","title":"Load Constant","text":"<pre><code>(\"load\", (\"const\", dest, value))\n# scratch[dest] = value (immediate constant)\n</code></pre>"},{"location":"notes/vliw-simd/instruction-set/#load-from-memory-scalar","title":"Load from Memory (Scalar)","text":"<pre><code>(\"load\", (\"load\", dest, addr_scratch))\n# scratch[dest] = memory[scratch[addr_scratch]]\n# Note: addr_scratch contains the ADDRESS, not the value\n</code></pre>"},{"location":"notes/vliw-simd/instruction-set/#load-from-memory-vector","title":"Load from Memory (Vector)","text":"<pre><code>(\"load\", (\"vload\", dest, addr_scratch))\n# scratch[dest:dest+8] = memory[addr:addr+8]\n# where addr = scratch[addr_scratch]\n# Loads 8 consecutive memory words\n</code></pre>"},{"location":"notes/vliw-simd/instruction-set/#store-engine-2-slotscycle","title":"Store Engine (2 slots/cycle)","text":"<p>Write to memory.</p>"},{"location":"notes/vliw-simd/instruction-set/#store-to-memory-scalar","title":"Store to Memory (Scalar)","text":"<pre><code>(\"store\", (\"store\", addr_scratch, src))\n# memory[scratch[addr_scratch]] = scratch[src]\n</code></pre>"},{"location":"notes/vliw-simd/instruction-set/#store-to-memory-vector","title":"Store to Memory (Vector)","text":"<pre><code>(\"store\", (\"vstore\", addr_scratch, src))\n# memory[addr:addr+8] = scratch[src:src+8]\n# where addr = scratch[addr_scratch]\n# Stores 8 consecutive memory words\n</code></pre>"},{"location":"notes/vliw-simd/instruction-set/#flow-engine-1-slotcycle","title":"Flow Engine (1 slot/cycle)","text":"<p>Control flow operations.</p>"},{"location":"notes/vliw-simd/instruction-set/#select-conditional-move","title":"Select (Conditional Move)","text":"<pre><code>(\"flow\", (\"select\", dest, cond, true_val, false_val))\n# scratch[dest] = scratch[true_val] if scratch[cond] else scratch[false_val]\n</code></pre>"},{"location":"notes/vliw-simd/instruction-set/#vector-select","title":"Vector Select","text":"<pre><code>(\"flow\", (\"vselect\", dest, cond, true_val, false_val))\n# For each i in 0..7:\n#   scratch[dest+i] = scratch[true_val+i] if scratch[cond+i] else scratch[false_val+i]\n</code></pre>"},{"location":"notes/vliw-simd/instruction-set/#jump-unconditional","title":"Jump (Unconditional)","text":"<pre><code>(\"flow\", (\"jump\", target_pc))\n# PC = target_pc (jump to instruction at index target_pc)\n</code></pre>"},{"location":"notes/vliw-simd/instruction-set/#conditional-jump","title":"Conditional Jump","text":"<pre><code>(\"flow\", (\"cond_jump\", cond_scratch, target_pc))\n# if scratch[cond_scratch]: PC = target_pc\n</code></pre>"},{"location":"notes/vliw-simd/instruction-set/#pause","title":"Pause","text":"<pre><code>(\"flow\", (\"pause\",))\n# Pause execution (for debugging synchronization)\n</code></pre>"},{"location":"notes/vliw-simd/instruction-set/#halt","title":"Halt","text":"<pre><code>(\"flow\", (\"halt\",))\n# Stop execution\n</code></pre>"},{"location":"notes/vliw-simd/instruction-set/#important-notes","title":"Important Notes","text":"<ol> <li>All effects apply at end of cycle - reads happen before writes</li> </ol> <pre><code># This works! Both read old values, then both write\n{ \"alu\": [(\"swap\", a, b, b), (\"swap\", b, a, a)] }  # Not actual syntax, just concept\n</code></pre> <ol> <li> <p>Scratch addresses must be allocated - Use <code>alloc_scratch()</code> in KernelBuilder</p> </li> <li> <p>Vector operations use contiguous addresses - <code>dest</code> means <code>dest</code> through <code>dest+7</code></p> </li> <li> <p>Memory addresses come from scratch - Load/store take a scratch address containing the memory address, not the memory address directly</p> </li> </ol>"},{"location":"notes/vliw-simd/optimization-strategies/","title":"Optimization Strategies","text":"<p>Techniques to reduce cycle count from the 147,734 baseline.</p>"},{"location":"notes/vliw-simd/optimization-strategies/#strategy-1-instruction-packing","title":"Strategy 1: Instruction Packing","text":"<p>Problem: Baseline puts one instruction per cycle.</p> <p>Solution: Pack multiple independent operations into the same cycle.</p>"},{"location":"notes/vliw-simd/optimization-strategies/#before-3-cycles","title":"Before (3 cycles)","text":"<pre><code>Cycle 1: { \"alu\": [(\"+\", 0, 1, 2)] }\nCycle 2: { \"alu\": [(\"*\", 3, 4, 5)] }\nCycle 3: { \"alu\": [(\"-\", 6, 7, 8)] }\n</code></pre>"},{"location":"notes/vliw-simd/optimization-strategies/#after-1-cycle","title":"After (1 cycle)","text":"<pre><code>Cycle 1: { \"alu\": [(\"+\", 0, 1, 2), (\"*\", 3, 4, 5), (\"-\", 6, 7, 8)] }\n</code></pre> <p>Potential speedup: Up to 12x for ALU-heavy code (12 ALU slots per cycle).</p>"},{"location":"notes/vliw-simd/optimization-strategies/#strategy-2-vectorization","title":"Strategy 2: Vectorization","text":"<p>Problem: Processing 1 item at a time when batch has 256 items.</p> <p>Solution: Use vector operations to process 8 items simultaneously.</p>"},{"location":"notes/vliw-simd/optimization-strategies/#before-scalar-8-items-8-cycles-minimum","title":"Before (scalar, 8 items = 8 cycles minimum)","text":"<pre><code>for i in range(8):\n    (\"alu\", (\"+\", result[i], a[i], b[i]))\n</code></pre>"},{"location":"notes/vliw-simd/optimization-strategies/#after-vector-8-items-1-cycle","title":"After (vector, 8 items = 1 cycle)","text":"<pre><code>(\"valu\", (\"+\", result_vec, a_vec, b_vec))  # All 8 at once\n</code></pre> <p>Potential speedup: 8x for data-parallel operations.</p>"},{"location":"notes/vliw-simd/optimization-strategies/#strategy-3-use-loops-instead-of-unrolling","title":"Strategy 3: Use Loops Instead of Unrolling","text":"<p>Problem: Baseline unrolls all 16 rounds x 256 items = 4,096 iterations as separate instructions.</p> <p>Solution: Use <code>cond_jump</code> to loop, reducing code size.</p>"},{"location":"notes/vliw-simd/optimization-strategies/#before-unrolled","title":"Before (unrolled)","text":"<pre><code># 4096 copies of the same instructions\nbody.append((\"alu\", (\"+\", ...)))  # item 0\nbody.append((\"alu\", (\"+\", ...)))  # item 1\nbody.append((\"alu\", (\"+\", ...)))  # item 2\n# ... 4093 more ...\n</code></pre>"},{"location":"notes/vliw-simd/optimization-strategies/#after-loop","title":"After (loop)","text":"<pre><code># Single copy of instructions, executed 4096 times\nloop_start = len(self.instrs)\nbody.append((\"alu\", (\"+\", ...)))       # The work\nbody.append((\"alu\", (\"+\", counter, counter, minus_one)))  # Decrement counter\nbody.append((\"flow\", (\"cond_jump\", counter, loop_start))) # Loop back\n</code></pre> <p>Benefits: Smaller code, fits in instruction cache, easier to reason about.</p>"},{"location":"notes/vliw-simd/optimization-strategies/#strategy-4-constant-caching","title":"Strategy 4: Constant Caching","text":"<p>Problem: Loading the same constant multiple times.</p> <p>Solution: Load constants once at startup, reuse from scratch.</p>"},{"location":"notes/vliw-simd/optimization-strategies/#before","title":"Before","text":"<pre><code>(\"load\", (\"const\", tmp, 2))  # Load 2\n(\"alu\", (\"*\", x, x, tmp))    # x * 2\n(\"load\", (\"const\", tmp, 2))  # Load 2 again!\n(\"alu\", (\"*\", y, y, tmp))    # y * 2\n</code></pre>"},{"location":"notes/vliw-simd/optimization-strategies/#after","title":"After","text":"<pre><code># Setup (once)\n(\"load\", (\"const\", two_const, 2))\n\n# Use (many times)\n(\"alu\", (\"*\", x, x, two_const))\n(\"alu\", (\"*\", y, y, two_const))\n</code></pre> <p>Note: <code>KernelBuilder.scratch_const()</code> already does this caching.</p>"},{"location":"notes/vliw-simd/optimization-strategies/#strategy-5-memory-access-batching","title":"Strategy 5: Memory Access Batching","text":"<p>Problem: Loading one value at a time.</p> <p>Solution: Use <code>vload</code>/<code>vstore</code> to transfer 8 values at once.</p>"},{"location":"notes/vliw-simd/optimization-strategies/#before-8-loads-8-cycles-minimum","title":"Before (8 loads = 8 cycles minimum)","text":"<pre><code>for i in range(8):\n    (\"load\", (\"load\", scratch[i], addr[i]))\n</code></pre>"},{"location":"notes/vliw-simd/optimization-strategies/#after-1-vload-1-cycle","title":"After (1 vload = 1 cycle)","text":"<pre><code>(\"load\", (\"vload\", scratch_base, addr_base))  # Loads scratch[base:base+8]\n</code></pre> <p>Constraint: Memory addresses must be consecutive.</p>"},{"location":"notes/vliw-simd/optimization-strategies/#strategy-6-pipeline-the-hash","title":"Strategy 6: Pipeline the Hash","text":"<p>Problem: Hash has 6 stages, each depending on the previous.</p> <p>Solution: While computing hash stage 3 for element A, compute stage 2 for element B, stage 1 for element C.</p>"},{"location":"notes/vliw-simd/optimization-strategies/#visualization","title":"Visualization","text":"<pre><code>Cycle 1: [A-stage1]\nCycle 2: [A-stage2] [B-stage1]\nCycle 3: [A-stage3] [B-stage2] [C-stage1]\nCycle 4: [A-stage4] [B-stage3] [C-stage2] [D-stage1]\n...\n</code></pre> <p>Complexity: High - requires careful register allocation and dependency tracking.</p>"},{"location":"notes/vliw-simd/optimization-strategies/#strategy-7-use-different-engines-in-parallel","title":"Strategy 7: Use Different Engines in Parallel","text":"<p>Problem: Only using ALU when you could also use VALU, Load, Store simultaneously.</p> <p>Solution: Schedule operations across engines.</p>"},{"location":"notes/vliw-simd/optimization-strategies/#before_1","title":"Before","text":"<pre><code>Cycle 1: { \"alu\": [compute1] }\nCycle 2: { \"load\": [load1] }\nCycle 3: { \"store\": [store1] }\n</code></pre>"},{"location":"notes/vliw-simd/optimization-strategies/#after_1","title":"After","text":"<pre><code>Cycle 1: { \"alu\": [compute1], \"load\": [load1], \"store\": [store1] }\n</code></pre>"},{"location":"notes/vliw-simd/optimization-strategies/#optimization-priority","title":"Optimization Priority","text":"<p>For this challenge, the most impactful optimizations are likely:</p> <ol> <li>Vectorization (8x potential) - Process 8 batch items at once</li> <li>Instruction packing (up to 23x potential) - Fill all engine slots</li> <li>Loops - Reduce code size, enable other optimizations</li> <li>Memory batching - Use vload/vstore</li> </ol> <p>The combination of vectorization + packing can theoretically achieve 100x+ speedup over the baseline.</p>"},{"location":"notes/vliw-simd/optimization-strategies/#data-dependencies-the-constraint","title":"Data Dependencies - The Constraint","text":"<p>You can only pack operations that are independent:</p> <pre><code># CANNOT pack - b depends on a\na = x + 1\nb = a * 2  # Must wait for 'a'\n\n# CAN pack - no dependencies\na = x + 1\nc = y + 1  # Independent of 'a'\n</code></pre> <p>Finding independent operations to pack is the core challenge of VLIW programming.</p>"},{"location":"notes/vliw-simd/scalar-vs-vector/","title":"Scalar vs Vector Operations","text":""},{"location":"notes/vliw-simd/scalar-vs-vector/#core-concept","title":"Core Concept","text":"<p>Scalar = One value at a time</p> <pre><code># Scalar: Process one number\na = 5\nb = a + 10  # Result: 15\n</code></pre> <p>Vector = Multiple values at once (in this architecture, 8 values)</p> <pre><code># Vector: Process 8 numbers simultaneously\na = [5, 6, 7, 8, 9, 10, 11, 12]      # 8 values\nb = a + 10                            # One instruction!\n# Result: [15, 16, 17, 18, 19, 20, 21, 22]\n</code></pre> <p>Why it matters: The vector operation takes 1 cycle to do what would take 8 cycles with scalar operations.</p>"},{"location":"notes/vliw-simd/scalar-vs-vector/#real-world-analogy","title":"Real-World Analogy","text":"<ul> <li>Scalar (cashier): One customer, scan items one by one</li> <li>Vector (warehouse): Forklift picks up 8 boxes at once</li> </ul>"},{"location":"notes/vliw-simd/scalar-vs-vector/#in-this-architecture","title":"In This Architecture","text":"Type Instruction What it does Scalar <code>(\"alu\", (\"+\", dest, a, b))</code> <code>dest = a + b</code> (one value) Vector <code>(\"valu\", (\"+\", dest, a, b))</code> <code>dest[0:8] = a[0:8] + b[0:8]</code> (8 values) Scalar Load <code>(\"load\", (\"load\", dest, addr))</code> Load 1 word from memory Vector Load <code>(\"load\", (\"vload\", dest, addr))</code> Load 8 consecutive words"},{"location":"notes/vliw-simd/scalar-vs-vector/#visual-example","title":"Visual Example","text":"<p>Adding 10 to values at memory addresses 100-107:</p>"},{"location":"notes/vliw-simd/scalar-vs-vector/#scalar-approach-8-cycles-minimum","title":"Scalar approach (8 cycles minimum)","text":"<pre><code>Cycle 1: load mem[100] -&gt; scratch[0]\nCycle 2: add scratch[0] + 10 -&gt; scratch[0]\nCycle 3: store scratch[0] -&gt; mem[100]\nCycle 4: load mem[101] -&gt; scratch[0]\nCycle 5: add scratch[0] + 10 -&gt; scratch[0]\nCycle 6: store scratch[0] -&gt; mem[101]\n... repeat for 102-107 ...\n</code></pre>"},{"location":"notes/vliw-simd/scalar-vs-vector/#vector-approach-3-cycles","title":"Vector approach (3 cycles)","text":"<pre><code>Cycle 1: vload mem[100:108] -&gt; scratch[0:8]     # Load all 8 at once\nCycle 2: vadd scratch[0:8] + 10 -&gt; scratch[0:8] # Add to all 8 at once\nCycle 3: vstore scratch[0:8] -&gt; mem[100:108]    # Store all 8 at once\n</code></pre>"},{"location":"notes/vliw-simd/scalar-vs-vector/#broadcasting","title":"Broadcasting","text":"<p>To add a scalar (like <code>10</code>) to a vector, you first need to \"broadcast\" it - copy the single value into all 8 slots of a vector:</p> <pre><code># scratch[16] contains the scalar value 10\n(\"valu\", (\"vbroadcast\", 24, 16))  # scratch[24:32] = [10,10,10,10,10,10,10,10]\n(\"valu\", (\"+\", 0, 0, 24))         # scratch[0:8] = scratch[0:8] + scratch[24:32]\n</code></pre> <p>This takes 2 cycles instead of 1, but you only broadcast once and can reuse the broadcasted vector for many operations.</p>"},{"location":"notes/vliw-simd/scalar-vs-vector/#key-takeaway","title":"Key Takeaway","text":"<p>Vectorization gives up to 8x speedup by processing 8 elements in the same time it takes to process 1.</p>"},{"location":"notes/vliw-simd/vliw-architecture/","title":"VLIW Architecture","text":""},{"location":"notes/vliw-simd/vliw-architecture/#vliw-very-long-instruction-word","title":"VLIW = Very Long Instruction Word","text":"<p>A CPU architecture where you (the programmer/compiler) explicitly schedule what runs in parallel, rather than the hardware figuring it out.</p>"},{"location":"notes/vliw-simd/vliw-architecture/#traditional-cpu-vs-vliw","title":"Traditional CPU vs VLIW","text":""},{"location":"notes/vliw-simd/vliw-architecture/#traditional-cpu-out-of-order-execution","title":"Traditional CPU (Out-of-Order Execution)","text":"<pre><code>You write:          CPU figures out:\nadd a, b, c         \"I can run these\nmul d, e, f          two in parallel\"\nsub g, h, i         \"This one waits\"\n</code></pre> <p>The hardware has complex circuits to detect parallelism at runtime.</p>"},{"location":"notes/vliw-simd/vliw-architecture/#vliw-cpu","title":"VLIW CPU","text":"<pre><code>You write exactly what happens each cycle:\n\nCycle 1: { alu: [add a,b,c], [mul d,e,f] }   # Both run together\nCycle 2: { alu: [sub g,h,i] }                # This runs alone\n</code></pre> <p>No complex hardware needed - you did the scheduling.</p>"},{"location":"notes/vliw-simd/vliw-architecture/#the-instruction-bundle","title":"The \"Instruction Bundle\"","text":"<p>In VLIW, each cycle executes an instruction bundle - a collection of operations across all engines:</p> <pre><code># One instruction bundle = one cycle\n{\n    \"alu\": [\n        (\"+\", 0, 1, 2),    # ALU operation 1\n        (\"*\", 3, 4, 5),    # ALU operation 2\n    ],\n    \"valu\": [\n        (\"+\", 16, 16, 24), # Vector operation\n    ],\n    \"load\": [\n        (\"load\", 10, 11),  # Memory load\n    ],\n    \"store\": [\n        (\"store\", 12, 13), # Memory store\n    ],\n    \"flow\": [\n        (\"select\", 6, 7, 8, 9),  # Conditional select\n    ]\n}\n</code></pre> <p>All of these execute simultaneously in one cycle.</p>"},{"location":"notes/vliw-simd/vliw-architecture/#why-vliw","title":"Why VLIW","text":""},{"location":"notes/vliw-simd/vliw-architecture/#advantages","title":"Advantages","text":"<ul> <li>Simpler hardware - No out-of-order execution logic needed</li> <li>Predictable timing - You know exactly how many cycles things take</li> <li>Power efficient - Less complex circuitry</li> </ul>"},{"location":"notes/vliw-simd/vliw-architecture/#disadvantages","title":"Disadvantages","text":"<ul> <li>Compiler/programmer burden - You must find the parallelism</li> <li>Code size - Bundles can have empty slots (NOPs)</li> <li>Less flexible - Can't adapt to runtime conditions</li> </ul>"},{"location":"notes/vliw-simd/vliw-architecture/#vliw-in-the-real-world","title":"VLIW in the Real World","text":"<ul> <li>Intel Itanium (IA-64) - Famous VLIW processor (discontinued)</li> <li>Texas Instruments DSPs - Audio/video processing</li> <li>Qualcomm Hexagon - Mobile DSP in Snapdragon chips</li> <li>GPUs (partially) - Some VLIW-like characteristics</li> </ul>"},{"location":"notes/vliw-simd/vliw-architecture/#in-this-challenge","title":"In This Challenge","text":"<p>The current baseline puts one operation per bundle:</p> <pre><code># Baseline: 3 cycles for 3 operations\nCycle 1: { \"alu\": [(\"+\", 0, 1, 2)] }\nCycle 2: { \"alu\": [(\"*\", 3, 4, 5)] }\nCycle 3: { \"alu\": [(\"-\", 6, 7, 8)] }\n</code></pre> <p>Optimized: 1 cycle for 3 operations (if they're independent):</p> <pre><code># Optimized: 1 cycle for 3 operations\nCycle 1: { \"alu\": [(\"+\", 0, 1, 2), (\"*\", 3, 4, 5), (\"-\", 6, 7, 8)] }\n</code></pre>"},{"location":"notes/vliw-simd/vliw-architecture/#key-insight","title":"Key Insight","text":"<p>The challenge is essentially: How well can you pack operations into bundles?</p> <ul> <li>Maximum ops per cycle: 12 ALU + 6 VALU + 2 Load + 2 Store + 1 Flow = 23 operations</li> <li>Baseline uses: ~1 operation per cycle</li> <li>Theoretical maximum speedup from packing alone: 23x</li> <li>Combined with vectorization (8x): theoretical 184x speedup</li> </ul> <p>Reality is lower due to data dependencies, but the gap between 147K cycles and 1.3K cycles shows what's possible.</p>"},{"location":"posts/","title":"Posts","text":"<p>Long-form essays and reflections.</p>"},{"location":"projects/","title":"Projects","text":"<p>Case studies, implementations, and builds.</p>"},{"location":"reading/","title":"Reading","text":"<p>Paper summaries, critiques, and reading notes.</p>"}]}