{"config":{"lang":["en"],"separator":"[\\s\\-\\.]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"<p>A knowledge base for AI/ML engineering - notes, labs, projects, and blog posts.</p>"},{"location":"#sections","title":"Sections","text":"<ul> <li>Notes - Concise explanations and concepts</li> <li>Labs - Hands-on experiments</li> <li>Projects - Case studies and builds</li> <li>Posts - Long-form essays</li> <li>Reading - Paper summaries and critiques</li> </ul>"},{"location":"labs/","title":"Labs","text":"<p>Hands-on experiments and Jupyter notebooks.</p>"},{"location":"notes/","title":"Notes","text":"<p>Concise explanations, definitions, and concepts.</p> <ul> <li>VLIW SIMD Architecture - Understanding VLIW architectures and SIMD optimization</li> </ul>"},{"location":"notes/vliw-simd/","title":"VLIW SIMD Architecture","text":"<p>VLIW (Very Long Instruction Word) and SIMD (Single Instruction, Multiple Data) are processor architecture techniques that enable parallel execution of multiple operations.</p> <p>This guide covers the key concepts needed to understand and optimize for VLIW SIMD architectures.</p>"},{"location":"notes/vliw-simd/#documents","title":"Documents","text":"<ol> <li> <p>Scalar vs Vector - Understanding the difference between processing one value vs eight values at once</p> </li> <li> <p>ALU and Engines - What ALU means and the five execution engines in this architecture</p> </li> <li> <p>VLIW Architecture - How Very Long Instruction Word architectures work</p> </li> <li> <p>Instruction Set Reference - Complete list of available instructions</p> </li> <li> <p>Optimization Strategies - Techniques to reduce cycle count</p> </li> </ol>"},{"location":"notes/vliw-simd/#key-concepts","title":"Key Concepts","text":"Concept Description VLIW Explicit instruction-level parallelism scheduled by compiler/programmer SIMD Data-level parallelism via vector operations Vector Length Number of elements processed simultaneously (commonly 4, 8, or 16) Instruction Bundle Group of operations executed in one cycle"},{"location":"notes/vliw-simd/#learning-path","title":"Learning Path","text":"<p>Start with Scalar vs Vector to understand data parallelism, then proceed through the documents in order.</p> <p>Related Project</p> <p>These notes were developed while studying the Anthropic Performance Take-Home challenge, which provides a hands-on way to apply these concepts.</p>"},{"location":"notes/vliw-simd/alu-and-engines/","title":"ALU and Execution Engines","text":""},{"location":"notes/vliw-simd/alu-and-engines/#alu-arithmetic-logic-unit","title":"ALU = Arithmetic Logic Unit","text":"<p>A fundamental CPU component that's been around since the 1940s. It's the part of a processor that does math and logic operations.</p>"},{"location":"notes/vliw-simd/alu-and-engines/#what-alu-handles","title":"What ALU Handles","text":"<ul> <li>Arithmetic: <code>+</code>, <code>-</code>, <code>*</code>, <code>//</code>, <code>%</code></li> <li>Logic: <code>&amp;</code> (AND), <code>|</code> (OR), <code>^</code> (XOR)</li> <li>Shifts: <code>&lt;&lt;</code>, <code>&gt;&gt;</code></li> <li>Comparisons: <code>&lt;</code>, <code>==</code></li> </ul> <p>Every CPU has an ALU. Your laptop, phone, even a calculator - all have ALUs.</p>"},{"location":"notes/vliw-simd/alu-and-engines/#valu-vector-alu","title":"VALU = Vector ALU","text":"<p>The vector version of ALU - does the same operations but on 8 values simultaneously.</p>"},{"location":"notes/vliw-simd/alu-and-engines/#typical-vliw-execution-units","title":"Typical VLIW Execution Units","text":"<p>Modern VLIW processors commonly include these types of execution units:</p> Unit Purpose Typical Parallelism ALU Scalar arithmetic and logic 2-16 operations/cycle VALU Vector arithmetic and logic 2-8 operations/cycle Load Memory reads 1-4 operations/cycle Store Memory writes 1-4 operations/cycle Flow Control flow (branches, jumps) 1 operation/cycle <p>The exact number of slots varies by processor. For example:</p> <ul> <li>TI C6x DSPs: 8 functional units (2 multipliers, 6 ALU-like)</li> <li>Qualcomm Hexagon: 4 execution slots per cycle</li> <li>Intel Itanium: Up to 6 instructions per bundle</li> </ul>"},{"location":"notes/vliw-simd/alu-and-engines/#what-slots-per-cycle-means","title":"What \"Slots per Cycle\" Means","text":"<p>Each engine can execute multiple operations per cycle (up to its slot limit):</p> <pre><code># This is ONE cycle - all 3 ALU operations happen simultaneously\n{\n    \"alu\": [\n        (\"+\", 0, 1, 2),   # scratch[0] = scratch[1] + scratch[2]\n        (\"-\", 3, 4, 5),   # scratch[3] = scratch[4] - scratch[5]\n        (\"*\", 6, 7, 8),   # scratch[6] = scratch[7] * scratch[8]\n    ]\n}\n</code></pre> <p>The ALU engine has 12 slots, so you could pack up to 12 independent ALU operations in a single cycle.</p>"},{"location":"notes/vliw-simd/alu-and-engines/#why-multiple-engines-matter","title":"Why Multiple Engines Matter","text":"<p>Different engines work in parallel. In one cycle you can:</p> <ul> <li>Do 12 ALU operations AND</li> <li>Do 6 VALU operations AND</li> <li>Do 2 loads AND</li> <li>Do 2 stores AND</li> <li>Do 1 flow operation</li> </ul> <p>The combined throughput across all engines represents the theoretical maximum operations per cycle. Achieving this requires finding enough independent operations to fill all slots.</p>"},{"location":"notes/vliw-simd/alu-and-engines/#real-cpus-use-these-terms","title":"Real CPUs Use These Terms","text":"<ul> <li>Intel/AMD x86: Has multiple ALUs per core</li> <li>GPUs (NVIDIA/AMD): Thousands of ALUs for parallel processing</li> <li>ARM: ALU is part of the core pipeline</li> </ul> <p>The terminology in this challenge mirrors real processor architecture.</p>"},{"location":"notes/vliw-simd/instruction-set/","title":"Instruction Set Reference","text":"<p>Common instruction patterns in VLIW SIMD architectures. Specific syntax varies by architecture, but the concepts are universal.</p>"},{"location":"notes/vliw-simd/instruction-set/#instruction-format","title":"Instruction Format","text":"<p>Every instruction is a tuple: <code>(engine, (operation, ...operands))</code></p> <p>Most operands are scratch addresses (not values). The scratch space is like a large register file.</p>"},{"location":"notes/vliw-simd/instruction-set/#alu-engine-12-slotscycle","title":"ALU Engine (12 slots/cycle)","text":"<p>Scalar arithmetic and logic operations.</p> <pre><code>(\"alu\", (op, dest, src1, src2))\n# scratch[dest] = scratch[src1] OP scratch[src2]\n</code></pre>"},{"location":"notes/vliw-simd/instruction-set/#available-operations","title":"Available Operations","text":"Op Description Example <code>+</code> Addition <code>scratch[0] = scratch[1] + scratch[2]</code> <code>-</code> Subtraction <code>scratch[0] = scratch[1] - scratch[2]</code> <code>*</code> Multiplication <code>scratch[0] = scratch[1] * scratch[2]</code> <code>//</code> Integer division <code>scratch[0] = scratch[1] // scratch[2]</code> <code>%</code> Modulo <code>scratch[0] = scratch[1] % scratch[2]</code> <code>&amp;</code> Bitwise AND <code>scratch[0] = scratch[1] &amp; scratch[2]</code> <code>\\|</code> Bitwise OR <code>scratch[0] = scratch[1] \\| scratch[2]</code> <code>^</code> Bitwise XOR <code>scratch[0] = scratch[1] ^ scratch[2]</code> <code>&lt;&lt;</code> Left shift <code>scratch[0] = scratch[1] &lt;&lt; scratch[2]</code> <code>&gt;&gt;</code> Right shift <code>scratch[0] = scratch[1] &gt;&gt; scratch[2]</code> <code>&lt;</code> Less than (returns 0 or 1) <code>scratch[0] = 1 if scratch[1] &lt; scratch[2] else 0</code> <code>==</code> Equal (returns 0 or 1) <code>scratch[0] = 1 if scratch[1] == scratch[2] else 0</code>"},{"location":"notes/vliw-simd/instruction-set/#valu-engine-6-slotscycle","title":"VALU Engine (6 slots/cycle)","text":"<p>Vector operations on VLEN=8 contiguous elements.</p> <pre><code>(\"valu\", (op, dest, src1, src2))\n# scratch[dest:dest+8] = scratch[src1:src1+8] OP scratch[src2:src2+8]\n</code></pre>"},{"location":"notes/vliw-simd/instruction-set/#available-operations_1","title":"Available Operations","text":"<p>Same as ALU (<code>+</code>, <code>-</code>, <code>*</code>, <code>//</code>, <code>%</code>, <code>&amp;</code>, <code>|</code>, <code>^</code>, <code>&lt;&lt;</code>, <code>&gt;&gt;</code>, <code>&lt;</code>, <code>==</code>) but operates on 8 elements.</p>"},{"location":"notes/vliw-simd/instruction-set/#special-broadcast","title":"Special: Broadcast","text":"<pre><code>(\"valu\", (\"vbroadcast\", dest, src))\n# scratch[dest:dest+8] = [scratch[src], scratch[src], ..., scratch[src]]\n# Copies one scalar value into all 8 vector slots\n</code></pre>"},{"location":"notes/vliw-simd/instruction-set/#load-engine-2-slotscycle","title":"Load Engine (2 slots/cycle)","text":"<p>Read from memory or load constants.</p>"},{"location":"notes/vliw-simd/instruction-set/#load-constant","title":"Load Constant","text":"<pre><code>(\"load\", (\"const\", dest, value))\n# scratch[dest] = value (immediate constant)\n</code></pre>"},{"location":"notes/vliw-simd/instruction-set/#load-from-memory-scalar","title":"Load from Memory (Scalar)","text":"<pre><code>(\"load\", (\"load\", dest, addr_scratch))\n# scratch[dest] = memory[scratch[addr_scratch]]\n# Note: addr_scratch contains the ADDRESS, not the value\n</code></pre>"},{"location":"notes/vliw-simd/instruction-set/#load-from-memory-vector","title":"Load from Memory (Vector)","text":"<pre><code>(\"load\", (\"vload\", dest, addr_scratch))\n# scratch[dest:dest+8] = memory[addr:addr+8]\n# where addr = scratch[addr_scratch]\n# Loads 8 consecutive memory words\n</code></pre>"},{"location":"notes/vliw-simd/instruction-set/#store-engine-2-slotscycle","title":"Store Engine (2 slots/cycle)","text":"<p>Write to memory.</p>"},{"location":"notes/vliw-simd/instruction-set/#store-to-memory-scalar","title":"Store to Memory (Scalar)","text":"<pre><code>(\"store\", (\"store\", addr_scratch, src))\n# memory[scratch[addr_scratch]] = scratch[src]\n</code></pre>"},{"location":"notes/vliw-simd/instruction-set/#store-to-memory-vector","title":"Store to Memory (Vector)","text":"<pre><code>(\"store\", (\"vstore\", addr_scratch, src))\n# memory[addr:addr+8] = scratch[src:src+8]\n# where addr = scratch[addr_scratch]\n# Stores 8 consecutive memory words\n</code></pre>"},{"location":"notes/vliw-simd/instruction-set/#flow-engine-1-slotcycle","title":"Flow Engine (1 slot/cycle)","text":"<p>Control flow operations.</p>"},{"location":"notes/vliw-simd/instruction-set/#select-conditional-move","title":"Select (Conditional Move)","text":"<pre><code>(\"flow\", (\"select\", dest, cond, true_val, false_val))\n# scratch[dest] = scratch[true_val] if scratch[cond] else scratch[false_val]\n</code></pre>"},{"location":"notes/vliw-simd/instruction-set/#vector-select","title":"Vector Select","text":"<pre><code>(\"flow\", (\"vselect\", dest, cond, true_val, false_val))\n# For each i in 0..7:\n#   scratch[dest+i] = scratch[true_val+i] if scratch[cond+i] else scratch[false_val+i]\n</code></pre>"},{"location":"notes/vliw-simd/instruction-set/#jump-unconditional","title":"Jump (Unconditional)","text":"<pre><code>(\"flow\", (\"jump\", target_pc))\n# PC = target_pc (jump to instruction at index target_pc)\n</code></pre>"},{"location":"notes/vliw-simd/instruction-set/#conditional-jump","title":"Conditional Jump","text":"<pre><code>(\"flow\", (\"cond_jump\", cond_scratch, target_pc))\n# if scratch[cond_scratch]: PC = target_pc\n</code></pre>"},{"location":"notes/vliw-simd/instruction-set/#pause","title":"Pause","text":"<pre><code>(\"flow\", (\"pause\",))\n# Pause execution (for debugging synchronization)\n</code></pre>"},{"location":"notes/vliw-simd/instruction-set/#halt","title":"Halt","text":"<pre><code>(\"flow\", (\"halt\",))\n# Stop execution\n</code></pre>"},{"location":"notes/vliw-simd/instruction-set/#important-notes","title":"Important Notes","text":"<ol> <li>All effects apply at end of cycle - reads happen before writes</li> </ol> <pre><code># This works! Both read old values, then both write\n{ \"alu\": [(\"swap\", a, b, b), (\"swap\", b, a, a)] }  # Not actual syntax, just concept\n</code></pre> <ol> <li> <p>Scratch addresses must be allocated - Track which addresses are in use</p> </li> <li> <p>Vector operations use contiguous addresses - <code>dest</code> means <code>dest</code> through <code>dest+7</code></p> </li> <li> <p>Memory addresses come from scratch - Load/store take a scratch address containing the memory address, not the memory address directly</p> </li> </ol>"},{"location":"notes/vliw-simd/optimization-strategies/","title":"Optimization Strategies","text":"<p>Techniques to reduce cycle count in VLIW SIMD architectures.</p>"},{"location":"notes/vliw-simd/optimization-strategies/#strategy-1-instruction-packing","title":"Strategy 1: Instruction Packing","text":"<p>Problem: Baseline puts one instruction per cycle.</p> <p>Solution: Pack multiple independent operations into the same cycle.</p>"},{"location":"notes/vliw-simd/optimization-strategies/#before-3-cycles","title":"Before (3 cycles)","text":"<pre><code>Cycle 1: { \"alu\": [(\"+\", 0, 1, 2)] }\nCycle 2: { \"alu\": [(\"*\", 3, 4, 5)] }\nCycle 3: { \"alu\": [(\"-\", 6, 7, 8)] }\n</code></pre>"},{"location":"notes/vliw-simd/optimization-strategies/#after-1-cycle","title":"After (1 cycle)","text":"<pre><code>Cycle 1: { \"alu\": [(\"+\", 0, 1, 2), (\"*\", 3, 4, 5), (\"-\", 6, 7, 8)] }\n</code></pre> <p>Potential speedup: Up to 12x for ALU-heavy code (12 ALU slots per cycle).</p>"},{"location":"notes/vliw-simd/optimization-strategies/#strategy-2-vectorization","title":"Strategy 2: Vectorization","text":"<p>Problem: Processing 1 item at a time when batch has many items.</p> <p>Solution: Use vector operations to process 8 items simultaneously.</p>"},{"location":"notes/vliw-simd/optimization-strategies/#before-scalar-8-items-8-cycles-minimum","title":"Before (scalar, 8 items = 8 cycles minimum)","text":"<pre><code>for i in range(8):\n    (\"alu\", (\"+\", result[i], a[i], b[i]))\n</code></pre>"},{"location":"notes/vliw-simd/optimization-strategies/#after-vector-8-items-1-cycle","title":"After (vector, 8 items = 1 cycle)","text":"<pre><code>(\"valu\", (\"+\", result_vec, a_vec, b_vec))  # All 8 at once\n</code></pre> <p>Potential speedup: 8x for data-parallel operations.</p>"},{"location":"notes/vliw-simd/optimization-strategies/#strategy-3-use-loops-instead-of-unrolling","title":"Strategy 3: Use Loops Instead of Unrolling","text":"<p>Problem: Unrolling all iterations as separate instructions bloats code size.</p> <p>Solution: Use <code>cond_jump</code> to loop, reducing code size.</p>"},{"location":"notes/vliw-simd/optimization-strategies/#before-unrolled","title":"Before (unrolled)","text":"<pre><code># 4096 copies of the same instructions\nbody.append((\"alu\", (\"+\", ...)))  # item 0\nbody.append((\"alu\", (\"+\", ...)))  # item 1\nbody.append((\"alu\", (\"+\", ...)))  # item 2\n# ... 4093 more ...\n</code></pre>"},{"location":"notes/vliw-simd/optimization-strategies/#after-loop","title":"After (loop)","text":"<pre><code># Single copy of instructions, executed 4096 times\nloop_start = len(self.instrs)\nbody.append((\"alu\", (\"+\", ...)))       # The work\nbody.append((\"alu\", (\"+\", counter, counter, minus_one)))  # Decrement counter\nbody.append((\"flow\", (\"cond_jump\", counter, loop_start))) # Loop back\n</code></pre> <p>Benefits: Smaller code, fits in instruction cache, easier to reason about.</p>"},{"location":"notes/vliw-simd/optimization-strategies/#strategy-4-constant-caching","title":"Strategy 4: Constant Caching","text":"<p>Problem: Loading the same constant multiple times.</p> <p>Solution: Load constants once at startup, reuse from scratch.</p>"},{"location":"notes/vliw-simd/optimization-strategies/#before","title":"Before","text":"<pre><code>(\"load\", (\"const\", tmp, 2))  # Load 2\n(\"alu\", (\"*\", x, x, tmp))    # x * 2\n(\"load\", (\"const\", tmp, 2))  # Load 2 again!\n(\"alu\", (\"*\", y, y, tmp))    # y * 2\n</code></pre>"},{"location":"notes/vliw-simd/optimization-strategies/#after","title":"After","text":"<pre><code># Setup (once)\n(\"load\", (\"const\", two_const, 2))\n\n# Use (many times)\n(\"alu\", (\"*\", x, x, two_const))\n(\"alu\", (\"*\", y, y, two_const))\n</code></pre> <p>Note: Many VLIW toolchains provide helpers for constant caching.</p>"},{"location":"notes/vliw-simd/optimization-strategies/#strategy-5-memory-access-batching","title":"Strategy 5: Memory Access Batching","text":"<p>Problem: Loading one value at a time.</p> <p>Solution: Use <code>vload</code>/<code>vstore</code> to transfer 8 values at once.</p>"},{"location":"notes/vliw-simd/optimization-strategies/#before-8-loads-8-cycles-minimum","title":"Before (8 loads = 8 cycles minimum)","text":"<pre><code>for i in range(8):\n    (\"load\", (\"load\", scratch[i], addr[i]))\n</code></pre>"},{"location":"notes/vliw-simd/optimization-strategies/#after-1-vload-1-cycle","title":"After (1 vload = 1 cycle)","text":"<pre><code>(\"load\", (\"vload\", scratch_base, addr_base))  # Loads scratch[base:base+8]\n</code></pre> <p>Constraint: Memory addresses must be consecutive.</p>"},{"location":"notes/vliw-simd/optimization-strategies/#strategy-6-pipeline-multi-stage-computations","title":"Strategy 6: Pipeline Multi-Stage Computations","text":"<p>Problem: Multi-stage computations where each stage depends on the previous create serial bottlenecks.</p> <p>Solution: While computing stage 3 for element A, compute stage 2 for element B, stage 1 for element C.</p>"},{"location":"notes/vliw-simd/optimization-strategies/#visualization","title":"Visualization","text":"<pre><code>Cycle 1: [A-stage1]\nCycle 2: [A-stage2] [B-stage1]\nCycle 3: [A-stage3] [B-stage2] [C-stage1]\nCycle 4: [A-stage4] [B-stage3] [C-stage2] [D-stage1]\n...\n</code></pre> <p>Complexity: High - requires careful register allocation and dependency tracking.</p>"},{"location":"notes/vliw-simd/optimization-strategies/#strategy-7-use-different-engines-in-parallel","title":"Strategy 7: Use Different Engines in Parallel","text":"<p>Problem: Only using ALU when you could also use VALU, Load, Store simultaneously.</p> <p>Solution: Schedule operations across engines.</p>"},{"location":"notes/vliw-simd/optimization-strategies/#before_1","title":"Before","text":"<pre><code>Cycle 1: { \"alu\": [compute1] }\nCycle 2: { \"load\": [load1] }\nCycle 3: { \"store\": [store1] }\n</code></pre>"},{"location":"notes/vliw-simd/optimization-strategies/#after_1","title":"After","text":"<pre><code>Cycle 1: { \"alu\": [compute1], \"load\": [load1], \"store\": [store1] }\n</code></pre>"},{"location":"notes/vliw-simd/optimization-strategies/#optimization-priority","title":"Optimization Priority","text":"<p>For most VLIW SIMD workloads, the most impactful optimizations are:</p> <ol> <li>Vectorization - Process multiple items at once (Nx speedup where N is vector length)</li> <li>Instruction packing - Fill all engine slots</li> <li>Loops - Reduce code size, enable other optimizations</li> <li>Memory batching - Use vector loads/stores</li> </ol> <p>The combination of vectorization + packing can achieve significant speedups over naive implementations.</p>"},{"location":"notes/vliw-simd/optimization-strategies/#applying-these-strategies","title":"Applying These Strategies","text":"<p>For a concrete example applying all these optimization strategies, see the Anthropic Performance Take-Home project, which demonstrates achieving 100x+ speedup on a VLIW SIMD architecture.</p>"},{"location":"notes/vliw-simd/optimization-strategies/#data-dependencies-the-constraint","title":"Data Dependencies - The Constraint","text":"<p>You can only pack operations that are independent:</p> <pre><code># CANNOT pack - b depends on a\na = x + 1\nb = a * 2  # Must wait for 'a'\n\n# CAN pack - no dependencies\na = x + 1\nc = y + 1  # Independent of 'a'\n</code></pre> <p>Finding independent operations to pack is the core challenge of VLIW programming.</p>"},{"location":"notes/vliw-simd/scalar-vs-vector/","title":"Scalar vs Vector Operations","text":""},{"location":"notes/vliw-simd/scalar-vs-vector/#core-concept","title":"Core Concept","text":"<p>Scalar = One value at a time</p> <pre><code># Scalar: Process one number\na = 5\nb = a + 10  # Result: 15\n</code></pre> <p>Vector = Multiple values at once (in this architecture, 8 values)</p> <pre><code># Vector: Process 8 numbers simultaneously\na = [5, 6, 7, 8, 9, 10, 11, 12]      # 8 values\nb = a + 10                            # One instruction!\n# Result: [15, 16, 17, 18, 19, 20, 21, 22]\n</code></pre> <p>Why it matters: The vector operation takes 1 cycle to do what would take 8 cycles with scalar operations.</p>"},{"location":"notes/vliw-simd/scalar-vs-vector/#real-world-analogy","title":"Real-World Analogy","text":"<ul> <li>Scalar (cashier): One customer, scan items one by one</li> <li>Vector (warehouse): Forklift picks up 8 boxes at once</li> </ul>"},{"location":"notes/vliw-simd/scalar-vs-vector/#common-simd-instruction-patterns","title":"Common SIMD Instruction Patterns","text":"Type Operation What it does Scalar <code>add dest, a, b</code> <code>dest = a + b</code> (one value) Vector <code>vadd dest, a, b</code> <code>dest[0:N] = a[0:N] + b[0:N]</code> (N values) Scalar Load <code>load dest, addr</code> Load 1 word from memory Vector Load <code>vload dest, addr</code> Load N consecutive words <p>Note: N is the vector length (commonly 4, 8, or 16 depending on architecture).</p>"},{"location":"notes/vliw-simd/scalar-vs-vector/#visual-example","title":"Visual Example","text":"<p>Adding 10 to values at memory addresses 100-107:</p>"},{"location":"notes/vliw-simd/scalar-vs-vector/#scalar-approach-8-cycles-minimum","title":"Scalar approach (8 cycles minimum)","text":"<pre><code>Cycle 1: load mem[100] -&gt; scratch[0]\nCycle 2: add scratch[0] + 10 -&gt; scratch[0]\nCycle 3: store scratch[0] -&gt; mem[100]\nCycle 4: load mem[101] -&gt; scratch[0]\nCycle 5: add scratch[0] + 10 -&gt; scratch[0]\nCycle 6: store scratch[0] -&gt; mem[101]\n... repeat for 102-107 ...\n</code></pre>"},{"location":"notes/vliw-simd/scalar-vs-vector/#vector-approach-3-cycles","title":"Vector approach (3 cycles)","text":"<pre><code>Cycle 1: vload mem[100:108] -&gt; scratch[0:8]     # Load all 8 at once\nCycle 2: vadd scratch[0:8] + 10 -&gt; scratch[0:8] # Add to all 8 at once\nCycle 3: vstore scratch[0:8] -&gt; mem[100:108]    # Store all 8 at once\n</code></pre>"},{"location":"notes/vliw-simd/scalar-vs-vector/#broadcasting","title":"Broadcasting","text":"<p>The problem: Vector ALUs operate on vectors, not mixed scalar+vector. You can't directly add a single number to a vector.</p> <pre><code>[5, 6, 7, 8, 9, 10, 11, 12] + 10 = ???  # Can't mix vector + scalar directly\n</code></pre> <p>The solution: \"Broadcast\" the scalar - replicate it into all slots of a vector first.</p> <pre><code>Step 1: Broadcast 10 \u2192 [10, 10, 10, 10, 10, 10, 10, 10]\n\nStep 2: Now both operands are vectors, so vector addition works:\n        [5, 6, 7, 8, 9, 10, 11, 12]\n      + [10, 10, 10, 10, 10, 10, 10, 10]\n      = [15, 16, 17, 18, 19, 20, 21, 22]\n</code></pre> <p>Why it's still fast: You broadcast once, then reuse that broadcasted vector for many operations. If you're adding <code>10</code> to 1000 different vectors, you broadcast once and use it 1000 times.</p>"},{"location":"notes/vliw-simd/scalar-vs-vector/#key-takeaway","title":"Key Takeaway","text":"<p>Vectorization gives up to 8x speedup by processing 8 elements in the same time it takes to process 1.</p>"},{"location":"notes/vliw-simd/vliw-architecture/","title":"VLIW Architecture","text":""},{"location":"notes/vliw-simd/vliw-architecture/#vliw-very-long-instruction-word","title":"VLIW = Very Long Instruction Word","text":"<p>A CPU architecture where you (the programmer/compiler) explicitly schedule what runs in parallel, rather than the hardware figuring it out.</p>"},{"location":"notes/vliw-simd/vliw-architecture/#traditional-cpu-vs-vliw","title":"Traditional CPU vs VLIW","text":""},{"location":"notes/vliw-simd/vliw-architecture/#traditional-cpu-out-of-order-execution","title":"Traditional CPU (Out-of-Order Execution)","text":"<pre><code>You write:          CPU figures out:\nadd a, b, c         \"I can run these\nmul d, e, f          two in parallel\"\nsub g, h, i         \"This one waits\"\n</code></pre> <p>The hardware has complex circuits to detect parallelism at runtime.</p>"},{"location":"notes/vliw-simd/vliw-architecture/#vliw-cpu","title":"VLIW CPU","text":"<pre><code>You write exactly what happens each cycle:\n\nCycle 1: { alu: [add a,b,c], [mul d,e,f] }   # Both run together\nCycle 2: { alu: [sub g,h,i] }                # This runs alone\n</code></pre> <p>No complex hardware needed - you did the scheduling.</p>"},{"location":"notes/vliw-simd/vliw-architecture/#the-instruction-bundle","title":"The \"Instruction Bundle\"","text":"<p>In VLIW, each cycle executes an instruction bundle - a collection of operations across all engines:</p> <pre><code># One instruction bundle = one cycle\n{\n    \"alu\": [\n        (\"+\", 0, 1, 2),    # ALU operation 1\n        (\"*\", 3, 4, 5),    # ALU operation 2\n    ],\n    \"valu\": [\n        (\"+\", 16, 16, 24), # Vector operation\n    ],\n    \"load\": [\n        (\"load\", 10, 11),  # Memory load\n    ],\n    \"store\": [\n        (\"store\", 12, 13), # Memory store\n    ],\n    \"flow\": [\n        (\"select\", 6, 7, 8, 9),  # Conditional select\n    ]\n}\n</code></pre> <p>All of these execute simultaneously in one cycle.</p>"},{"location":"notes/vliw-simd/vliw-architecture/#why-vliw","title":"Why VLIW","text":""},{"location":"notes/vliw-simd/vliw-architecture/#advantages","title":"Advantages","text":"<ul> <li>Simpler hardware - No out-of-order execution logic needed</li> <li>Predictable timing - You know exactly how many cycles things take</li> <li>Power efficient - Less complex circuitry</li> </ul>"},{"location":"notes/vliw-simd/vliw-architecture/#disadvantages","title":"Disadvantages","text":"<ul> <li>Compiler/programmer burden - You must find the parallelism</li> <li>Code size - Bundles can have empty slots (NOPs)</li> <li>Less flexible - Can't adapt to runtime conditions</li> </ul>"},{"location":"notes/vliw-simd/vliw-architecture/#vliw-in-the-real-world","title":"VLIW in the Real World","text":"<ul> <li>Intel Itanium (IA-64) - Famous VLIW processor (discontinued)</li> <li>Texas Instruments DSPs - Audio/video processing</li> <li>Qualcomm Hexagon - Mobile DSP in Snapdragon chips</li> <li>GPUs (partially) - Some VLIW-like characteristics</li> </ul>"},{"location":"notes/vliw-simd/vliw-architecture/#key-insight","title":"Key Insight","text":"<p>The fundamental challenge in VLIW programming is: How efficiently can you pack operations into instruction bundles?</p> <ul> <li>More operations per bundle = fewer cycles = better performance</li> <li>The limiting factors are:</li> <li>Engine slot limits (how many operations of each type per cycle)</li> <li>Data dependencies (operations that depend on each other cannot run in parallel)</li> <li>Available independent work in the algorithm</li> </ul> <p>VLIW architectures shift the burden of finding parallelism from hardware (at runtime) to the compiler/programmer (at compile time).</p>"},{"location":"notes/vliw-simd/vliw-architecture/#applying-vliw-concepts","title":"Applying VLIW Concepts","text":"<p>For a hands-on example applying these concepts, see the Anthropic Performance Take-Home project.</p>"},{"location":"posts/","title":"Posts","text":"<p>Long-form essays and reflections.</p>"},{"location":"projects/","title":"Projects","text":"<p>Case studies, implementations, and builds.</p> <ul> <li>Anthropic Performance Take-Home - VLIW SIMD optimization challenge</li> </ul>"},{"location":"projects/anthropic-performance-takehome/","title":"Anthropic Performance Take-Home","text":"<p>A performance optimization challenge for a custom VLIW SIMD architecture simulator, originally used by Anthropic for technical interviews.</p>"},{"location":"projects/anthropic-performance-takehome/#overview","title":"Overview","text":"<p>This challenge involves optimizing a kernel that traverses binary trees while performing hash operations. The goal is to minimize execution cycles on a simulated VLIW SIMD processor.</p>"},{"location":"projects/anthropic-performance-takehome/#architecture-specifications","title":"Architecture Specifications","text":"<p>The challenge uses a custom VLIW SIMD architecture with five execution engines:</p> Engine Slots/Cycle Purpose ALU 12 Scalar arithmetic VALU 6 Vector arithmetic (VLEN=8) Load 2 Memory reads Store 2 Memory writes Flow 1 Control flow <p>Key Parameters:</p> <ul> <li>SCRATCH_SIZE: 1536 32-bit words (register file)</li> <li>Vector Length (VLEN): 8 elements per vector operation</li> <li>Batch Size: 256 items processed per round</li> <li>Rounds: 16 iterations total</li> </ul>"},{"location":"projects/anthropic-performance-takehome/#performance-benchmarks","title":"Performance Benchmarks","text":"Cycles Description 147,734 Baseline (unoptimized scalar) 18,532 Updated starting point (2-hour version) 1,790 Best human ~2hr / Claude Opus 4.5 casual 1,487 Impressive threshold 1,363 Best known (Claude Opus 4.5 improved harness)"},{"location":"projects/anthropic-performance-takehome/#the-problem","title":"The Problem","text":"<p>Each round processes a batch of 256 items through these steps:</p> <ol> <li>Load index and value from memory</li> <li>XOR value with tree node value at index</li> <li>Apply 6-stage hash function</li> <li>Branch left (<code>idx*2+1</code>) if hash is even, else right (<code>idx*2+2</code>)</li> <li>Wrap to root if past tree bounds</li> <li>Store updated index and value</li> </ol>"},{"location":"projects/anthropic-performance-takehome/#optimization-strategies-applied","title":"Optimization Strategies Applied","text":"<p>The optimization journey from 147K to ~1.4K cycles involves:</p> <ol> <li>Vectorization (8x) - Process 8 batch items simultaneously using VALU</li> <li>Instruction Packing (up to 23x) - Fill all engine slots per cycle</li> <li>Loops - Replace unrolled code with <code>cond_jump</code> loops</li> <li>Memory Batching - Use <code>vload</code>/<code>vstore</code> for 8-element transfers</li> <li>Constant Caching - Pre-load constants to scratch space</li> <li>Hash Pipelining - Overlap hash stages across elements</li> </ol> <p>Combined theoretical speedup: 100x+ (8x vectorization * ~13x packing efficiency)</p>"},{"location":"projects/anthropic-performance-takehome/#background-knowledge","title":"Background Knowledge","text":"<p>For the foundational concepts needed to understand this challenge, see the VLIW SIMD Architecture Notes:</p> <ul> <li>Scalar vs Vector - Understanding data parallelism</li> <li>ALU and Engines - Execution units</li> <li>VLIW Architecture - Instruction bundling</li> <li>Instruction Set - Available operations</li> <li>Optimization Strategies - General techniques</li> </ul>"},{"location":"projects/anthropic-performance-takehome/#key-commands","title":"Key Commands","text":"<pre><code># Run submission tests (official validation)\npython tests/submission_tests.py\n\n# Run cycle count test\npython perf_takehome.py Tests.test_kernel_cycles\n\n# Generate and view trace\npython perf_takehome.py Tests.test_kernel_trace\npython watch_trace.py  # Open browser, click \"Open Perfetto\"\n\n# Validate tests unchanged (important!)\ngit diff origin/main tests/\n</code></pre>"},{"location":"projects/anthropic-performance-takehome/#lessons-learned","title":"Lessons Learned","text":"<ol> <li>VLIW forces explicit parallelism - You must manually schedule what runs together</li> <li>Data dependencies are the bottleneck - Finding independent operations is key</li> <li>Vectorization is powerful but constrained - Requires contiguous memory access patterns</li> <li>Instruction packing requires careful planning - Scratch space allocation matters</li> </ol>"},{"location":"reading/","title":"Reading","text":"<p>Paper summaries, critiques, and reading notes.</p>"}]}